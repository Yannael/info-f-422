{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical foundations of Machine Learning INFO-F-422\n",
    "\n",
    "\n",
    "## TP 2 - Estimation\n",
    "\n",
    "#### *Yann-AÃ«l Le Borgne, Fabrizio Carcillo and Gianluca Bontempi*\n",
    "\n",
    "####  March 14, 2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic notions\n",
    "\n",
    "* Estimation: it is the procedure which allows to *estimate* a parameter of a distribution (expected value, variance, ...) from $N$ samples drawn from this distribution.\n",
    "* Typical estimators of the expected value and the variance are given by the sample mean\n",
    "$$\n",
    " \\hat{\\mu}=\\frac{1}{N}\\sum_{i=1}^N z_i\\\\\n",
    "$$\n",
    "and sample variance\n",
    "$$\n",
    "\\hat{\\sigma}^2= \\frac{1}{N-1}\\sum_{i=1}^N (z_i-\\hat{\\mu})^2,\n",
    "$$\n",
    "where $D_N=\\{z_1,\\ldots,z_n\\}$ is our sampleset.\n",
    "* An estimator $\\hat{\\boldsymbol{\\theta}}$ is a random variable itself, since it depends on a random sample $\\mathbf{D}_N$.\n",
    "* An estimator $\\hat{\\boldsymbol{\\theta}}$ of a parameter $\\theta$ is called unbiased if and only if\n",
    "\n",
    "\\begin{equation}\n",
    " {E}_{\\boldsymbol{D}_N}[\\hat{\\boldsymbol{\\theta}}]=\\theta.\n",
    "\\end{equation}\n",
    "\n",
    "If not, we define the *bias* as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{Bias}[\\hat{\\boldsymbol{\\theta}}]={E}_{\\boldsymbol{D}_N}[\\hat{\\boldsymbol{\\theta}}]-\\theta.\n",
    "\\end{equation}\n",
    "*  The variance of an estimator is defined as\n",
    "\\begin{equation}\n",
    " \\mbox{Var}[\\hat{\\boldsymbol{\\theta}}]={E}_{\\boldsymbol{D}_N}[(\\hat{\\boldsymbol{\\theta}}-E[\\hat{\\boldsymbol{\\theta}}])^2].\n",
    "\\end{equation}\n",
    "*  Bias and variance of $\\hat{\\mathbf{\\mu}}$:\n",
    "\\begin{equation}\n",
    " {E}_{\\boldsymbol{D}_N}[\\hat{\\boldsymbol{\\mu}}]=\\mu.\n",
    "\\end{equation}\n",
    "The estimator $\\hat{\\boldsymbol{\\mu}}$ is therefore unbiased and its variance is\n",
    "\\begin{equation}\n",
    "  \\mbox{Var}[\\hat{\\boldsymbol{\\mu}}]=\\frac{\\sigma^2}{N}.\n",
    "\\end{equation}\n",
    "where $\\mbox{Var}[{\\mathbf{z}}]=\\sigma^2$.\n",
    "\n",
    "*  Bias of $\\hat{\\boldsymbol{\\sigma}}^2$:\n",
    "\\begin{equation}\n",
    " E_{\\boldsymbol{D}_N}[\\hat{\\boldsymbol{\\sigma}}^2]=\\sigma^2.\n",
    "\\end{equation}\n",
    "The estimator $\\hat{\\boldsymbol{\\sigma}}^2$ is thus unbiased.\n",
    "\n",
    "*  The quality of an estimator $\\hat{\\boldsymbol{\\theta}}$ can be measured using the *mean square error*\n",
    "\n",
    "\\begin{equation}\n",
    " \\mbox{MSE}={E}_{\\boldsymbol{D}_N}[(\\theta - \\hat{\\boldsymbol{\\theta}})^2].\n",
    "\\end{equation}\n",
    "We can show that for all estimators $\\hat{\\boldsymbol{\\theta}}$\n",
    "\\begin{equation}\n",
    " \\mbox{MSE}=\\mbox{Var}[\\hat{\\boldsymbol{\\theta}}]+({E}[\\hat{\\boldsymbol{\\theta}}]-\\theta)^2.\n",
    "\\end{equation}\n",
    "is the sum of the variance and the squared bias.\n",
    "*  Let $\\hat{F}_z(x)=\\frac{1}{z}\\sum_{i=1}^z \\mathbb{1}_{x_i\\le t}$ be the empirical distribution function. We have\n",
    "\\begin{equation}\n",
    " {E}_{\\boldsymbol{D}_N}[\\hat{\\bf F}_z(x)]=F_z(x),\n",
    "\\end{equation}\n",
    "where $F_z(x)$ is the distribution function of the variable $\\boldsymbol{z}$.\n",
    "\n",
    "*  Let $N$ observations be drawn form a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. The estimator $\\hat{\\boldsymbol{\\mu}}$ of the mean  follows a normal distribution with mean $\\mu$ and standard deviation $\\sigma/\\sqrt{N}$. It follows that a confidence interval for $\\mu$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    " \\mbox{Prob}\\left\\{ \\hat{\\boldsymbol{\\mu}}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}} \\le \\mu\\le \\hat{\\boldsymbol{\\mu}}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}}\\right\\}=1-\\alpha,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is directly related to the probability $P$ that the interval contains $\\mu$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical experiments \n",
    "\n",
    "The R programs are written in files with the extension '.R', which can be edited using text editors such as emacs, gedit, etc. The file can be loaded in the R terminal with the command\n",
    "\n",
    "```\n",
    "source(\"filename.R\")\n",
    "```\n",
    "\n",
    "The additional parameter *print.eval=T* forces all outputs of the scripts to be displayed on the screen: \n",
    "```\n",
    "source(\"filename.R\", print.eval=T)\n",
    "```\n",
    "\n",
    "You can directly change into the directory containing the scripts using the command \n",
    "```\n",
    "setwd(\"directory containing the scripts\")\n",
    "```\n",
    "The goal of this TP is to write script for the following exercises. \n",
    "\n",
    "You can use the scripts *cumdis.R, cumdis_2.R, sam_dis.R, sam_dis2.R, sam_dis_unif.R, mse_bv.R, combine.R* and *confidence.R* to help you with the exercises. The scripts are available on the homepage of the course: https://github.com/gbonte/gbcode/tree/master/inst/scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution function \n",
    "\n",
    "Write a script that displays the empirical cumulative distribution function of a distribution $\\mathcal{N}(1,2)$ with 100 observations. Use the functions *ecdf* and *rnorm*. See [cumdis.R](https://github.com/gbonte/gbcode/tree/master/inst/scripts/cumdis.R).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample<-rnorm(100,mean=1,sd=sqrt(2))\n",
    "ecdf_obj<-ecdf(sample)\n",
    "plot(ecdf_obj,main=\"empirical cdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected value of the empirical distribution function\n",
    "\n",
    "Write a script which verifies the assertion ${E}_{\\mathbf{D}_N}[\\hat{\\bf F}_z(x)]=F_z(x)$ concerning the cumulative empirical distribution function. Modify the previous code in order to\n",
    "\n",
    "* generate $R$ samples of 100 observations\n",
    "* average the $R$ empirical cdfs\n",
    "* trace the distribution function of the sample mean and compare it with the theoretical distribution function.\n",
    "* observe the results for $R\\in \\{5,10,50,100\\}.$\n",
    "\n",
    "See [cumdis_2.R](https://github.com/gbonte/gbcode/tree/master/inst/scripts/cumdis_2.R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N<-100\n",
    "R<-c(5,10,50,100)\n",
    "I<-seq(-5,5,by=.1)\n",
    "emp<-NULL\n",
    "for (i in 1:100){\n",
    "  DN<-rnorm(N)\n",
    "  F<-ecdf(DN)\n",
    "  emp<-rbind(emp,F(I))\n",
    "}\n",
    "\n",
    "par(mfrow=c(2,2))\n",
    "\n",
    "for (i in 1:4) {\n",
    "    m.emp<-apply(emp[1:R[i],],2,mean)\n",
    "    plot(I,m.emp,pch=16,col=\"blue\",cex=0.5,main=paste(\"Average of \",i,\" empirical distributions\"))\n",
    "    lines(I,pnorm(I),pch=15) #  distribution function\n",
    "    legend(-4,1,legend=c(\"Empirical\",\"Gaussian\"),lty=c(3,1),cex=0.8,col=c(\"black\",\"blue\"))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator of the mean\n",
    "\n",
    "Write a script which returns 1000 estimations of the sample's mean using $N$ observations following a normal distribution $\\mathcal{N}(0,100)$. Une $N=50$, $N=75$ and $N=100$. Plot the histogram of these estimations and compare this with the theoretical distribution of the mean's estimator. \n",
    "\n",
    "Make use of the script [sam_dis.R](https://github.com/gbonte/gbcode/tree/master/inst/scripts/sam_dis.R) which allows to see in practice how the estimator $\\hat{\\boldsymbol{\\mu}}$ of the mean is distributed for a normal distribution of the data. \n",
    "\n",
    "Observe that it is unbiased and that its distribution is $\\mathcal{N}(\\mu, \\sigma^2/N)$ (have a look at the observed variances and at the shape of the histograms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "  \n",
    "mu<-0\n",
    "sdev<-10\n",
    "I<-seq(-50,50,by=.5)\n",
    "p<-dnorm(I,mean=mu,sd=sdev)\n",
    "plot(I,p,type=\"l\", main=paste(\"Distribution of  r.v. z: mean=\",mu,\"var=\",sdev^2),cex.main=0.9)\n",
    "\n",
    "R<-1000\n",
    "\n",
    "for (N in c(10,100)) {\n",
    "    \n",
    "    samples<-matrix(0,nr=R,nc=N)\n",
    "      for(i in 1:R){\n",
    "        samples[i,]<-rnorm(N,mean=mu,sd=sdev)\n",
    "      }\n",
    "\n",
    "    mu.hat<-apply(samples,1,mean)\n",
    "    hist(mu.hat,freq=FALSE, main= paste(\"Size N=\",N, \": variance of estimator=\",round(var(mu.hat),digits=3)),xlim=c(min(I),max(I)),cex.main=0.9)\n",
    "    p.mu.hat<-dnorm(I,mean=mean(mu.hat),sd=sqrt(var(mu.hat)))\n",
    "    lines(I,p.mu.hat,type=\"l\",col=\"blue\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator of the variance\n",
    "\n",
    "Proceed equivalently except that here the estimator of the variance is considered. We want to verify that $\\frac{(N-1)\\hat{\\boldsymbol{\\sigma}}^2}{\\sigma^2}\\sim \\chi^2_{N-1}$. Use $N=10$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "  \n",
    "mu<-0\n",
    "sdev<-10\n",
    "I<-seq(-50,50,by=.5)\n",
    "p<-dnorm(I,mean=mu,sd=sdev)\n",
    "plot(I,p,type=\"l\", main=paste(\"Distribution of  r.v. z: mean=\",mu,\"var=\",sdev^2),cex.main=0.9)\n",
    "\n",
    "R<-1000\n",
    "\n",
    "for (N in 10){\n",
    "    \n",
    "    samples<-matrix(0,nr=R,nc=N)\n",
    "      for(i in 1:R){\n",
    "        samples[i,]<-rnorm(N,mean=mu,sd=sdev)\n",
    "      }\n",
    "\n",
    "    var.hat<-apply(samples,1,var)\n",
    "    hist(var.hat,freq=FALSE, main= paste(\"Size N=\",N, \": variance of estimator=\",round(var(var.hat),digits=3)),cex.main=0.9)\n",
    "    \n",
    "    I2<-seq(0,1500,by=.5)\n",
    "    ch<-(var.hat*(N-1))/(sdev^2)\n",
    "    hist(ch,freq=FALSE,cex.main=0.9,main=\"Histogram of chi squared\")\n",
    "    p.var.hat<-dchisq(I2,df=N-1)\n",
    "    lines(I2,p.var.hat,type=\"l\")\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and variance\n",
    "\n",
    "Write a script which verifies the equation\n",
    "\n",
    "\\begin{equation}\n",
    " \\mbox{MSE}=\\mbox{Var}[\\hat{\\boldsymbol{\\theta}}]+({E}[\\hat{\\boldsymbol{\\theta}}]-\\theta)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Take as an example the estimator of the mean of 10 observations following the distribution $\\mathcal{N}(0,100)$, by generating 10000 estimations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N<-10\n",
    "mu<-0\n",
    "sdev<-10\n",
    "R<-10000\n",
    "\n",
    "\n",
    "I<-seq(-50,50,.5)\n",
    "p<-dnorm(I,mean=mu,sd=sdev)\n",
    "plot(I,p,type=\"l\",\n",
    "     main=paste(\"Distribution of  r.v. z: var=\",sdev^2),cex.main=0.7)\n",
    "\n",
    "mu.hat<-array(0,dim=c(R,1))\n",
    "for (r in 1:R){\n",
    "\n",
    "  D<-rnorm(N,mean=mu,sd=sdev)\n",
    "  \n",
    "  mu.hat[r,1]<-mean(D)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err<-mu.hat-mu\n",
    "\n",
    "MSE<-mean(err^2)\n",
    "BIAS<-mean(mu.hat)-mu\n",
    "VARIANCE<-mean((mu.hat-mean(mu.hat))^2)\n",
    "\n",
    "paste(MSE)\n",
    "paste(BIAS^2)\n",
    "paste(VARIANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of estimators\n",
    "\n",
    "The mean of unbiased estimators, having the same variance is itself unbiased but has a variance twice smaller than that of the estimators it has been derived from (see slide 48 in the file http://www.ulb.ac.be/di/map/gbonte/mod_stoch/nonlin.pdf). Write a script which illustrates this by\n",
    "\n",
    "* generating independently two distributions of the estimator of the mean for a uniform distribution assuming values between -10 and 10,\n",
    "* displaying the histograms of the two distributions and the combination of both and compute their variance.\n",
    "\n",
    "The corresponding script is *combine.R*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set.seed(2)\n",
    "\n",
    "sampling1<-array(runif(1000,-10,10),c(100,10))\n",
    "sampling2<-array(runif(1000,-10,10),c(100,10))\n",
    "\n",
    "meanSampling1<-apply(sampling1,1,mean)\n",
    "meanSampling2<-apply(sampling2,1,mean)\n",
    "\n",
    "meanSampling1_2<-(meanSampling1+meanSampling2)/2\n",
    "\n",
    "paro<-par(mfrow=c(2,2))\n",
    "hist(meanSampling1,main=paste(\"Variance first estimator: \",round(var(meanSampling1),digits=3),sep=\"\"),cex.main=0.9,ylim=c(0,30))\n",
    "hist(meanSampling2,main=paste(\"Variance second estimator: \",round(var(meanSampling2),digits=3),sep=\"\"),cex.main=0.9,ylim=c(0,30))\n",
    "hist(meanSampling1_2,main=paste(\"Variance combination of estimators: \",round(var(meanSampling1_2),digits=3),sep=\"\"),cex.main=0.9,ylim=c(0,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "Write a script which generates $N$ samples of the distribution $\\mathcal{N}(1,5)$, returns the percentage of values not falling into the $P\\%$ confidence interval. Test, using $P=95\\%$ with $N=100$ and $N=1000$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paro<-par(mfrow=c(2,2))\n",
    "\n",
    "mu<-1\n",
    "sdev<-sqrt(5)\n",
    "\n",
    "alpha<-0.05\n",
    "z.alpha<-qnorm(alpha/2, lower=FALSE)\n",
    "\n",
    "for (N in c(100,1000)){\n",
    "    D<-rnorm(N,mean=mu,sd=sigma)\n",
    "    perc<-mean((D<(mu-z.alpha*sdev))| (D>(mu+z.alpha*sdev)))\n",
    "    hist(D,main=paste(\"Percentage outside confidence interval (%): \",round(perc,digits=3),sep=\"\"),cex.main=0.9 )\n",
    "    abline(v=mu-z.alpha*sdev)\n",
    "    abline(v=mu+z.alpha*sdev)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
